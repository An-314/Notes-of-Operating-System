#import "@preview/scripst:1.1.1": *

= 多处理器调度

== 对称多处理与多核架构

#note(subname: [问题])[
  多处理器与单处理器系统有什么异同？
  - 优势：处理能力加倍
  - 挑战：处理器间的通信和协调难度和开销也加倍
  #three-line-table[
    | 特征         | 单处理器系统               | 多处理器系统                |
    | ---------- | -------------------- | --------------------- |
    | *CPU 数量* | 一个处理器执行所有任务          | 两个及以上处理器同时执行任务        |
    | *并行性*    | 无真正并行，仅靠时间片轮转实现“伪并行” | 多核可实现真正的指令级并行执行       |
    | *调度复杂度*  | 任务调度简单，只需考虑单核队列      | 调度算法更复杂，要考虑核间负载均衡     |
    | *通信*     | 所有任务共享同一内存、无通信延迟问题   | 核间通信（cache、总线）需要协调一致性 |
    | *容错性*    | 单点故障风险高              | 一核失效时，其他核可继续运行，提高可靠性  |
    | *性能扩展*   | 受制于单核频率提升瓶颈          | 可通过增加核心数线性提升吞吐量（有限度）  |
  ]
  - 多处理器系统的优势
    - 性能提升
    - 系统可靠性提高
    - 更高的吞吐量
  - 多处理器系统的挑战
    - 处理器间的通信与同步
    - 调度与负载均衡
    - 资源共享问题
    - 系统设计复杂
  - *对称多处理结构（SMP：Symmetric Multiprocessing）*
    - 每个处理器地位平等，共享同一主内存、I/O、操作系统。
  - *多核架构（Multicore Architecture）*
    - 在单个芯片上集成多个处理核心（Core），每个核心拥有独立的寄存器和执行单元，但共享部分缓存或内存。
]

=== 多处理机器

*单核处理器（Single-core CPU）*
- 典型的单核处理器架构：
  - 寄存器文件（Register File）：保存当前指令操作数和中间结果
  - 算术逻辑单元（ALU）：执行加减、逻辑等运算
  - 总线接口（Bus Interface）：连接系统总线，与主存、I/O设备通信
  - I/O桥（I/O Bridge）：连接系统总线、I/O总线与内存总线
  - 外设控制器（USB、磁盘、显示适配器等）：负责设备的输入输出
- 要点总结：
  - 单核一次只能执行一个线程（通过时间片轮转实现多任务“伪并行”）
  - 性能提升主要依赖于主频与流水线深度
  - 缺点：主频提升受制于功耗与发热瓶颈
  #figure(
    image("pic/2025-10-29-14-22-53.png", width: 80%),
    numbering: none,
  )
*超线程（Hyper-Threading / Simultaneous Multithreading, SMT）处理器*
- 现代 CPU 内部有多个执行单元（整数单元、浮点单元等），有时并未充分利用。超线程技术通过在一个物理核心上同时运行两个（或多个）线程，让不同类型的指令流共享硬件资源，提高执行单元利用率。
- 将CPU内部暂时闲置处理资源充分调动起来
  - 寄存器、程序计数器独立
  - 算术计算单元等硬件共用
  - 不适合计算密集型任务
  - 适合IO密集型任务
  #figure(
    image("pic/2025-10-29-14-19-49.png", width: 80%),
    numbering: none,
  )
*多核处理器（Multi-core Processor）*
- 单个芯片上集成多个独立的处理核心（Core）；
  - 每个核心拥有：
    - 自己的寄存器文件
    - 自己的 ALU
    - 独立的一级缓存
    - 共享部分资源（如二级缓存、总线接口）
- 优势：
  - 多个核心可真正并行执行任务
  - 适合并行计算与多任务环境
  - 在相同功耗下比提升主频更有效率
- 挑战：
  - 核间通信与同步（缓存一致性问题）
  - 调度复杂度增加
  - 共享总线带宽成为瓶颈
*众核(many-core)处理器*

*对称多处理器(SMP)与非一致内存访问系统(NUMA)*
- SMP（Symmetric Multiprocessing）
  - 所有 CPU 共享同一主存
  - 每个 CPU 都能访问任何内存单元
  - 系统简单、均衡
  - 但共享总线成为瓶颈
- NUMA（Non-Uniform Memory Access）
  - 每个 CPU 拥有本地内存
  - 访问本地内存速度快，访问远程内存速度慢
  - 通过互连（Router）协调各节点通信
  - 适用于大规模服务器与多路系统
  #figure(
    image("pic/2025-10-29-14-26-55.png", width: 80%),
    numbering: none,
  )

=== Cache一致性(Cache Coherence)

在多处理器系统中（SMP 或 多核架构），每个 CPU/Core 都有自己的 高速缓存（Cache），用于加速数据访问。

多个处理器共享同一主内存，但各自维护本地缓存副本。当一个处理器修改了缓存中的数据，而其他处理器的缓存中仍是旧值，就会出现数据不一致（Inconsistency）。

*Cache 一致性问题的本质*
- Cache 一致性问题属于“共享变量的可见性问题”
  #three-line-table[
    | 层级       | 问题描述           |
    | -------- | -------------- |
    | *缓存级别* | 每个核心的数据副本可能不同步 |
    | *内存级别* | 内存与缓存数据不一致     |
    | *并发级别* | 线程读到旧数据（失效缓存）  |
  ]
- 解决目标是：让所有核心看到的共享数据始终一致（Coherent）
*Cache 一致性协议（Coherence Protocols）*
- 典型协议：MESI 协议
  #three-line-table[
    | 状态                | 含义    | 说明                     |
    | ----------------- | ----- | ---------------------- |
    | *M (Modified)*  | 缓存已修改 | 此缓存块已被修改，和内存不一致；独占该缓存块 |
    | *E (Exclusive)* | 独占    | 与内存一致，未被其他核心缓存         |
    | *S (Shared)*    | 共享    | 与内存一致，多个核心缓存同一数据       |
    | *I (Invalid)*   | 无效    | 该缓存块内容无效，需重新加载         |
  ]
  写失效（Write Invalidate）策略
  - 当一个核心写入数据时，其他核心的缓存副本将被标记为无效。
- 总线监听机制（Bus Snooping） 或 目录协议（Directory-based Coherence）
  - 核心通过监听总线或查询目录来保持缓存一致性。

== 多处理器调度概述

#note(subname: [问题])[
  如何多处理器调度中的各处理器关系？
  - 紧密耦合
  - 相对独立
  #three-line-table[
    | 类型                        | 特征                       | 代表结构                |
    | ------------------------- | ------------------------ | ------------------- |
    | *紧密耦合（Tightly Coupled）* | 多个处理器共享内存、操作系统与 I/O 资源   | 对称多处理系统（SMP）、多核处理器  |
    | *相对独立（Loosely Coupled）* | 各处理器有独立内存、操作系统，通过消息或网络通信 | 集群系统（Cluster）、分布式系统 |
  ]
  #three-line-table[
    | 比较项   | 紧密耦合系统 (SMP/多核)   | 相对独立系统 (Cluster/分布式) |
    | ----- | ----------------- | -------------------- |
    | 内存访问  | 共享主存              | 各节点独立内存              |
    | 通信方式  | 内存共享              | 消息传递                 |
    | 操作系统  | 单一 OS 内核          | 各节点独立 OS             |
    | 调度策略  | 全局/分区/混合调度        | 节点级调度 + 全局任务分配       |
    | 数据一致性 | Cache 一致性协议（MESI） | 软件一致性或复制机制           |
    | 典型结构  | 多核 CPU、SMP 服务器    | 集群、云服务器              |
    | 优点    | 响应快、共享数据高效        | 扩展性好、容错性强            |
    | 缺点    | 可扩展性有限（总线瓶颈）      | 通信开销大、延迟高            |
  ]
]


=== 单队列多处理器调度SQMS

*单队列多处理器调度*Single Queue Multiprocessor Scheduling, SQMS
- 复用单处理器调度下的基本架构
- 所有需要调度的进程放入一个队列中
  #figure(
    image("pic/2025-10-29-16-20-30.png", width: 80%),
    numbering: none,
  )
- 特征
  - 缺乏可扩展性 (scalability)
    - 随着 CPU 核数增加，多个 CPU 会同时访问同一个就绪队列
    - 需要加锁（Lock）保护
    - 导致锁竞争严重、调度开销显著上升
    - 多核扩展能力差
  - 缓存亲和性 (cache affinity) 弱
    - 任务可能每次都在不同 CPU 上运行
    - 导致频繁的缓存失效（cache miss）
    - 降低整体性能

*多处理器调度的亲和度与负载均衡*
- 尽可能让进程在同一个 CPU 上运行
- 保持一些进程的亲和度的同时，可能需要牺牲其他进程的亲和度来实现负载均衡
  #figure(
    image("pic/2025-10-29-16-28-12.png", width: 80%),
    numbering: none,
  )

*QMS 调度流程示意*
- 所有任务进入全局队列
- 各 CPU 从该队列中取出一个任务
- CPU 执行完任务后，再回队列取下一个
- 调度器通过全局锁维护队列一致性

=== 多队列多处理器调度MQMS

*多队列多处理器调度*Multi-Queue MultiprocessorScheduling, MQMS
- 基本调度框架包含*多个调度队列*，每个队列可用不同的调度规则
- 依照一些启发性规则，在进程进入系统时，将其放入某个调度队列
- *每个 CPU 调度相互独立*，避免单队列方式的数据共享及同步问题
  #figure(
    image("pic/2025-10-29-16-40-11.png", width: 80%),
    numbering: none,
  )
  这说明：
  - 每个 CPU 拥有自己的就绪队列
  - 各队列相互独立
  - 调度器在本地队列上做决策（例如 RR、FCFS）
  - 不需要频繁访问全局共享数据结构
  #figure(
    image("pic/2025-10-29-16-40-26.png", width: 80%),
    numbering: none,
  )
- *多队列多处理器调度的特征*
  - 具有*可扩展性*：队列的数量会随着CPU 的增加而增加，因此锁和缓存争用的开销不是大问题
  - 具有良好的*缓存亲和度*：所有进程都保持在固定的 CPU 上，因而可以很好地利用缓存数据
- *多队列多处理器调度的负载不均*
  - 假定4个进程，2个CPU；队列都执行轮转调度策略
  - 进程C执行完毕
  - A 获得了 B 和 D 两倍的 CPU 时间
  #figure(
    image("pic/2025-10-29-16-45-20.png", width: 80%),
    numbering: none,
  )
- 如何解决MQMS的负载不均？
  - 假定 4 个进程，2 个 CPU；每个队列都执行轮转调度策略；A 和 C 都执行完毕，系统中只有 B 和 D
    - CPU1 很忙
    - CPU0 空闲
    #figure(
      image("pic/2025-10-29-16-46-19.png", width: 80%),
      numbering: none,
    )
*进程迁移 (migration)*
- 通过进程的跨 CPU 迁移，可实现负载均衡
  - 情况：CPU0 空闲，CPU1 有一些进程
  - 迁移：将 B 或 D 迁移到 CPU0
- MQMS如何确定进程迁移时机?
  - 情况：A 独自留在 CPU 0 上，B 和 D 在 CPU 1 上交替运行
  - 迁移：不断地迁移和切换一个或多个进程
*MQMS的工作窃取 (work stealing)*
- 进程量较少的 (源) 队列不定期地“偷看”其他 (目标) 队列是不是比自己的进程多
- 如果目标队列比源队列 (显著地) 更满，就从目标队列“窃取”一个或多个进程，实现负载均衡
  #figure(
    image("pic/2025-10-29-16-47-41.png", width: 80%),
    numbering: none,
  )
  #figure(
    image("pic/2025-10-29-16-47-47.png", width: 80%),
    numbering: none,
  )
- 工作窃取的队列检查间隔
  - 如果频繁地检查其他队列，就会带来较高的开销，可扩展性不好
  - 如果检查间隔太长，又可能会带来严重的负载不均

#three-line-table[
  | 对比维度  | SQMS（单队列） | MQMS（多队列）                |
  | ----- | --------- | ------------------------ |
  | 调度队列  | 1个全局队列    | 每CPU一个独立队列               |
  | 锁竞争   | 高         | 低                        |
  | 缓存亲和性 | 弱         | 强                        |
  | 可扩展性  | 差         | 好                        |
  | 负载均衡  | 全局自然均衡    | 需通过迁移或窃取机制实现             |
  | 典型应用  | 小型多核系统    | 多核服务器、现代操作系统（如Linux CFS） |
]

#note(subname: [小结])[
  多处理器调度
  - 紧密耦合：单队列多处理器调度
    - 调度队列的互斥访问
    - 高速缓存命中率下降
  - 相对独立：多队列多处理器调度
    - 每个处理器的调度相对独立
    - 需要协调处理器间的负载均衡
]

== Linux O(1) 调度

=== SMP 和 早期Linux 内核

*Linux调度器的演进*
#three-line-table[
  | 阶段             | 调度器类型                                               | 内核版本           | 复杂度      | 特点               |
  | -------------- | --------------------------------------------------- | -------------- | -------- | ---------------- |
  | *老 (Old)*    | O(n) 调度器                                            | 2.4 \~ 2.6      | O(n)     | 每次调度需遍历所有进程，性能差  |
  | *中 (Middle)* | O(1) 调度器                                            | 2.6.0 \~ 2.6.22 | O(1)     | 调度操作时间固定，与进程数量无关 |
  | *青 (New)*    | CFS 调度器（Completely Fair Scheduler）                  | 2.6.23 \~ 6.5   | O(log n) | 完全公平调度，基于红黑树     |
  | *最新*         | EEVDF 调度器（Earliest Eligible Virtual Deadline First） | 6.6+ (2023起)   | O(log n) | 改进CFS的延迟控制与公平性   |
]

#newpara()

*调度器需要考虑的关键问题*
- 采用何种*数据结构*来组织进程
- 如何根据进程优先级来确定*进程运行时间*
- 如何判断*进程类型*(I/O密集、CPU密集型；实时、非实时)
- 如何确定进程的动态*优先级*：影响因素
  - 静态优先级、nice值
  - I/O密集型和CPU密集型产生的优先级奖惩
- 如何适配*多处理器情况*

*SMP 和 早期Linux 内核*
- Linux 1.2
  - 环形队列 + Round Robin调度策略
  - 仅支持单CPU
  - 所有任务共享一个调度队列
- Linux 2.0
  - SMP（Symmetric Multi-Processing）支持
  - SMP 支持由一个“大锁”组成，“大锁”对内核访问串行化
  - 在用户态支持并行，Linux 内核本身并不能利用多处理器加速
- Linux 2.2
  - 引入调度类（real-time, non-real-time）

=== Linux O(n)调度器

*O(n) 调度器概述*
- 使用版本：Linux 内核 2.4 ~ 2.6.0
- 名称由来：
  - O(n) 表示调度算法的时间复杂度与活跃进程数 n 成正比
  - 即：每次调度决策都要遍历所有可运行的进程（O(n) 扫描）
  #figure(
    image("pic/2025-10-29-17-18-03.png", width: 80%),
    numbering: none,
  )
*Linux O(n) 调度算法的思路*
- 时间片机制（Epoch）
  - 系统把时间划分为多个周期（Epoch），在每个周期开始时
  - 为每个进程分配一个“时间片（time slice）”
  - 时间片大小根据进程优先级决定（优先级高 → 时间片多）
- 调度流程
  - 在每次调度时，调度器扫描所有就绪进程；
  - 找出*动态优先级*最高的那个
  - 将进程优先级映射成缺省时间片，让它运行，直到时间片耗尽
  - 然后选择优先级最高的进程来执行
  - 若某进程提前阻塞（例如等待I/O），则未使用完的时间片可以累积到下一次
- 进程被调度器切换执行后，可不被打扰地*用尽这个时间片*
- 如进程没有用尽时间片，则*剩余时间*增加到进程的下一个时间片中
*算法复杂度：O(n)*
- 每次调度时，调度器都要：
  - 遍历系统中*所有活跃进程*
  - 比较它们的优先级
  - 选择最高者执行
*Linux O(n)调度器数据结构*
- 只用一个 global runqueue放置就绪任务
- 各个 core 需要竞争同一个 runqueue 里面的任务
  #figure(
    image("pic/2025-10-29-17-23-59.png", width: 80%),
    numbering: none,
  )
  #three-line-table[
    | 组件                        | 说明                                |
    | ------------------------- | --------------------------------- |
    | *Global runnable queue* | 全局就绪队列，保存系统所有可运行进程（t₁, t₂, t₃, …） |
    | *Single spinlock*       | 所有CPU访问队列时需竞争这一个锁                 |
    | *Scheduler logic*       | 调度器核心逻辑：扫描队列、选择进程、分配CPU           |
    | *Multiple CPUs*         | 所有CPU共享这一调度逻辑与队列                  |
  ]
*O(n) 调度器的主要缺点*
- O(n)执行开销
  - 当有大量进程在运行时，这个调度器的性能将会被大大降低
- 多处理器*竞争访问*同一个 runqueue 里面的任务
  - O(n)调度器没有很好的可扩展性(scalability)

=== Linux O(1) 调度器

*Linux O(1) 调度器*
- Linux 2.6 版本的调度器是由 Ingo Molnar 设计并实现的
- 为唤醒、上下文切换和定时器中断开销建立一个完全 O(1) 的调度器
- 调度器的 选择（select）、插入（enqueue）、删除（dequeue） 等操作，无论系统中有多少个进程，都能在常数时间内完成

*Linux O(1) 调度器的思路*
- 实现了per-cpu-runqueue，每个CPU都有一个就绪进程任务队列
- 采用全局优先级
  - 实时进程0-99
  - 普通进程100-139
  #figure(
    image("pic/2025-10-29-17-35-49.png", width: 80%),
    numbering: none,
  )
  - 各 CPU 独立调度，无需全局锁
  - 可扩展性高
  - 负载不均时，触发任务迁移（migration）
  - 实现多核系统的高效并行调度
- 双数组结构：Active / Expired Queue
  - 活跃数组active：放置就绪进程 —— 存放可立即执行的进程，正在运行中
  - 过期数组expire：放置过期进程 —— 存放时间片用完的进程，等待重新调度
  - 当 active 队列空时，交换两者角色
    - 实现了任务的“轮换”和“动态优先级再平衡”
  #figure(
    image("pic/2025-10-29-17-39-25.png", width: 80%),
    numbering: none,
  )
- 核心数据结构细节
  - 优先级映射
    - Linux 将进程优先级划分为 140 个等级：
      - 实时任务：0–99
      - 普通任务：100–139
  - 每个优先级对应一个链表
    - 每个链表中保存相同优先级的任务（FIFO队列）
  - 位图（Bitmap）
    - 用 140-bit 位图记录当前哪些优先级队列非空
    - 通过查找最高优先级位（left-most bit=1）快速找到可运行任务
    - 使用单条 CPU 指令（bsf/bsr）即可定位 → 时间复杂度 O(1)
  - O(1) 调度算法流程
    - 在 active 位图中寻找最高优先级 bit
    - 从对应优先级队列取出队头进程（FIFO）
    - 运行进程，直到时间片耗尽或阻塞
    - 若时间片用尽 → 重新计算优先级 → 放入 expired 队列
    - 若 active 队列空 → 交换 active 与 expired

  #figure(
    image("pic/2025-10-29-17-42-11.png", width: 80%),
    numbering: none,
  )

*常用数据结构访问的时间复杂度*
- 常用数据结构访问的时间复杂度
  #three-line-table[
    | 操作        | 实现机制            | 复杂度  |
    | --------- | --------------- | ---- |
    | 选择最高优先级进程 | 位图扫描 (bit scan) | O(1) |
    | 插入进程      | 链表插入尾部          | O(1) |
    | 删除进程      | 链表头部取出          | O(1) |
    | 队列交换      | 指针切换            | O(1) |
  ]
  - access：随机访问
    - array: 平均情况和最坏情况均能达到 O(1)
    - linked list 是 O(N)
    - tree 一般是 O(log N)
  - search：搜索
    - hash table 时间复杂度是 O(1)，但它最坏情况下是 O(N)
    - 大部分 tree（b-tree / red-black tree）平均和最坏情况都是 O(log N)
  - insert/deletion：插入和删除
    - hash table 时间复杂度是 O(1)，但它最坏情况下是 O(N)
    - linked list，stack，queue 在平均和最坏情况下都是 O(1)
- 进程有 140 种优先级，可用长度为 140 的数组去记录优先级
  - access 是 O(1)
- 位图bitarray为每种优先级分配一个 bit
  - 如果这个优先级队列下面有进程，那么就对相应的 bit 染色，置为 1，否则置为 0
  - 问题简化为寻找位图中最高位是 1 的 bit（left-most bit），可用一条CPU 指令实现
- 每个优先级下面用一个FIFO queue 管理这个优先级下的进程
  - 新来的插到队尾，先进先出，insert/deletion 都是 O(1)

*LinuxO(1)活跃数组和过期数组*
- 活跃数组(Active Priority Array, APA)
- 过期数组(Expired Priority Array, EPA)
  - 在 active bitarray 中寻找 left-most bit 的位置 x
  - 在 APA 中找到对应队列 APA[x]
  - 从 队列APA[x] 中取出一个进程
  - 对于当前执行完的进程，重新计算其优先级，然后 放入到 EPA 相应的队列EPA[priority]
  - 如果进程优先级在 expired bitarray 里对应的 bit 为 0，将其置 1
  - 如果 active bitarray 全为零，将 active bitarray 和 expired bitarray 交换

*Linux O(1) 调度器的多核/SMP支持*
- *按固定时间间隔执行负载均衡（Load Balancing）*
  - 在每个时钟中断后进行计算CPU负载
  - 检查各CPU runqueue的负载
  - 如果某CPU空闲，则从繁忙CPU拉取任务（pulling）
  - 避免使用 pushing 模式（由繁忙CPU主动推任务）

== Linux CFS 调度

*完全公平调度(CFS, Completely Fair Scheduler)*

#note(subname: [问题])[
  这一节是理解现代 Linux 调度器的关键，它回答了一个核心问题：*如何让每个进程按比例（fairly）地分配 CPU 资源？*

  理想的“完全公平”CPU模型
  - 假设系统有$N$个可运行的进程，权重为$omega_i$，则
    - 进程$i$在任意时间段$T$内应获得的CPU时间为：
      $
        T_i = T (omega_i / sum omega_j)
      $
    - 其中，$sum omega_j$是所有可运行进程的权重之和
]

=== CFS的原理

*CFS的背景*
- O(1)和O(n)都将CPU资源划分为时间片
  - 采用*固定额度*分配机制，每个调度周期的进程可用时间片是确定的
  - 调度周期结束被重新分配
- O(1)调度器本质上是*MLFQ（multi-level feedback queue）*算法思想
- 不足：O(1)调度器对进程交互性的*响应不及时*
- 需求
  - 根据*进程的运行状况*判断它属于IO密集型还是CPU密集型，再做优先级奖励和惩罚
  - 这种推测本身存在误差，场景越复杂判断难度越大

*CFS 的思路*
- 摒弃固定时间片分配，采用*动态时间片分配*
- 每次调度中进程可占用的时间与进程总数、总CPU时间、进程权重等均有关系，每个调度周期的值都可能会不一样
- 每个进程都有一个nice值, 表示其静态优先级
  ```c
  static const int prio_to_weight[40] = {
  /* -20 */ 88761, 71755, 56483, 46273, 36291,
  /* -15 */ 29154, 23254, 18705, 14949, 11916,
  /* -10 */ 9548, 7620, 6100, 4904, 3906,
  /*  -5 */ 3121, 2501, 1991, 1586, 1277,
  /*   0 */ 1024, 820, 655, 526, 423,
  /*   5 */ 335, 272, 215, 172, 137,
  /*  10 */ 110, 87, 70, 56, 45,
  /*  15 */ 36, 29, 23, 18, 15,
  };
  ```
  说明：
  - Linux 中 nice 值范围：-20 \~ +19
  - 共 40 个等级
  - 每提升一级（nice 值增加 1），权重大约减少 10%
  - 这意味着进程的执行权重呈几何递减关系，而非线性变化

*CFS 调度思想*
- 虚拟运行时间（vruntime）
  - CFS 为每个进程维护一个 vruntime 值，该值表示*进程应该获得的 CPU 时间量*
- 进程调度
  - 每次*调度vruntime最小的进程*，使得每个进程根据vruntime相互追赶，期望每个进程vruntime接近，保证公平
- *vruntime增长速度*
  - 进程的 nice 值（反映进程的相对优先级）会影响其 vruntime 的增长速率
  - nice值越低，权重越高，其vruntime增加得越慢
- 时间片管理
  - CFS 为每个进程分配一个时间片，当进程用完其时间片时，会被放回就绪队列的末尾
  - 时间片大小会根据系统的负载和进程的权重动态调整

*CFS 的进程运行时间动态分配*
- 根据各个进程的优先级权重分配运行时间
  - 进程权重越大，分到的运行时间越多
  ```
  分配给进程的运行时间 = 调度周期 * 进程权重 / 所有进程权重总和
  ```
- 调度周期
  - 将所有处于 `TASK_RUNNING` 态进程都调度一遍的时间
*CFS 的虚拟时间vruntime*
- virtual runtime(vruntime)：记录着进程已经运行的时间
  - vruntime是根据进程的权重将运行时间放大或者缩小一个比例
  ```
  vruntime = 实际运行时间 * 1024 / 进程权重
  ```
  - 1024是nice为0的进程的权重，代码中是`NICE_0_LOAD`
  - 所有进程都以nice为0的进程的权重1024作为基准，计算自己的vruntime增加速度
- 以上面A和B两个进程为例，B的权重是A的2倍，那么B的vruntime增加速度只有A的一半
  ```
  vruntime = (调度周期 * 进程权重 / 所有进程总权重) * 1024 / 进程权重
         = 调度周期 * 1024 / 所有进程总权重
  ```
  虽然进程的权重不同，但它们期望的vruntime应该是一样的，与权重无关
- *CFS 的虚拟时间计算*
  - 所有进程的vruntime增长速度宏观上看应该是同时推进的，就可以用这个vruntime来选择运行的进程
  - 进程的vruntime值较小说明它以前占用cpu的时间较短，受到了“不公平”对待，因此下一个运行进程就选择它
  - 这样既能*公平*选择进程，又能保证高*优先级*进程获得较多的运行时间
  - CFS让每个调度实体（进程或进程组）的vruntime互相追赶，而每个调度实体的vruntime增加速度不同，权重越大的增加的越慢，这样就能获得更多的cpu执行时间
  - 示例
    ```
    A每6个时间片执行1个时间片，B每3个时间片执行1个时间片，C每2个时间片执行1个时间片
    vruntime：
    A:   0  6  6  6  6  6  6  12 12 12 12 12 12
    B:   0  0  3  3  3  6  6  6  9  9  9  12 12
    C:   0  0  0  2  4  4  6  6  6  8  10 10 12
    调度：   A  B  C  C  B  C  A  B  C  C   B  C
    ```

=== CFS的实现

*红黑树：CFS中进程vruntime数据结构*
- 每个 CPU 都有独立的 CFS 运行队列
  ```c
  struct cfs_rq {
      struct rb_root_cached tasks_timeline; // 红黑树
      u64 min_vruntime;                     // 最小虚拟时间
      ...
  };
  ```
- Linux 采用了红黑树记录下每一个进程的 vruntime
  - 在多核系统中，每个核一棵红黑树
  - 调度时，从红黑树中选取vruntime最小的进程出来运行
  #figure(
    image("pic/2025-10-29-18-33-55.png", width: 80%),
    numbering: none,
  )
  - 树节点按 vruntime 排序
  - 最左节点是 vruntime 最小的任务
  - 调度时直接取最左节点 → O(1)
*CFS 的进程权重*
- 权重由 nice 值确定，权重跟进程 nice 值一一对应
- nice值越大，权重越低
- 通过全局数组 `prio_to_weight` 来转换
*新进程的 vruntime 初始化*
- 如果新建进程的 vruntime = 0，那它会远小于其他老进程 → 导致长期霸占CPU
- 每个 CPU 的运行队列 cfs_rq 都维护一个min_vruntime 字段
  - 记录该运行队列中所有进程的 vruntime 最小值
- 新进程的初始 vruntime 值设置为它所在运行队列的min_vruntime
  - 与老进程保持在合理的差距范围内
*休眠与唤醒机制*
- CFS中休眠进程的 vruntime 一直保持不变吗
  - 如果休眠进程的 vruntime 保持不变
  - 其他任务的 vruntime 仍在继续增加，
  - 唤醒后该进程的 vruntime 会变得“过小”，导致立即抢占 CPU
- 在休眠进程被唤醒时重新设置 vruntime 值，以 min_vruntime 值为基础，给予一定的*补偿*，但不能补偿太多
- CFS中休眠进程在唤醒时会立刻抢占 CPU 吗？
  - 休眠进程在醒来的时候有能力抢占 CPU 是*大概率事件*，这也是 CFS 调度算法的本意，即保证交互式进程的响应速度
  - *交互式进程*等待用户输入会频繁休眠
  - *主动休眠的进程*同样也会在唤醒时获得补偿，这类进程往往并不要求快速响应，它们同样也会在每次唤醒并抢占，这有可能会导致其它更重要的应用进程被抢占，有损整体性能
  - `sched_features` 的 `WAKEUP_PREEMPT` 位表示禁用唤醒抢占特性，刚唤醒的进程*不立即抢占*运行中的进程，而是要等到运行进程用完时间片之后

*进程迁移与多核调度*
- 每个队列中的进程的 vruntime 也走得有快有慢，每个CPU运行队列的 min_vruntime 值，都会有不同
- *迁移时做 vruntime 偏移校正*
  - 当任务从 CPU A 迁移到 CPU B 时
    ```
    离开A时:  vruntime -= A.min_vruntime
    加入B时:  vruntime += B.min_vruntime
    ```
  - 这保证迁移后：
    - 任务在新队列中能公平竞争
    - 不会因时间偏移获得过多或过少的 CPU 时间

*vruntime 溢出问题*
- `vruntime` 类型为` unsigned long`，随着系统长期运行，它可能增长到极大值并发生回绕溢出
- 比较 vruntime 时，不直接比较绝对值，而比较相对差值
  ```c
  signed long delta = (long)(vruntime_a - vruntime_b);
  if (delta < 0)
      a 比 b 小；
  else
      a 比 b 大；
  ```
  这种做法即使发生溢出，也能保证判断结果正确

*CFS 的公平调度逻辑总结*
- #three-line-table[
    | 操作阶段     | 机制                        |
    | -------- | ------------------------- |
    | *新建进程* | `vruntime = min_vruntime` |
    | *休眠进程* | vruntime 暂停增长             |
    | *唤醒进程* | 参考 min_vruntime，适度补偿      |
    | *任务迁移* | vruntime 校正（减旧队列、加新队列）    |
    | *溢出检测* | 用有符号差值比较 vruntime         |
    | *调度选择* | 红黑树中 vruntime 最小的进程       |
  ]

*Linux 调度器的模块化优点*
- #three-line-table[
    | 模块                   | 功能                 |
    | -------------------- | ------------------ |
    | *Main Scheduler*   | 负责选择下一个任务          |
    | *Tick Scheduler*   | 定期触发调度决策           |
    | *Load Balancer*    | 多核任务迁移与负载均衡        |
    | *Context Switch*   | 执行上下文切换            |
    | *CFS Scheduler*    | 普通任务调度，核心红黑树机制     |
    | *RT Scheduler*     | 实时任务，优先级高          |
    | *DL Scheduler*     | Deadline 调度，满足时限需求 |
    | *Idle Scheduler*   | 空闲CPU时运行idle任务     |
    | *Custom Scheduler* | 用户可定制调度策略          |
  ]

*Linux 调度器的模块化结构*
#figure(
  image("pic/2025-10-29-18-47-01.png", width: 80%),
  numbering: none,
)

Core Scheduler（核心调度框架） 负责全局的调度管理，包括：
- Main scheduler：调度逻辑核心
- Tick scheduler：周期性触发调度
- Load balancer：跨CPU负载均衡
- Context Switch：上下文切换
- CPU Runqueue (rbtree)：各CPU上的运行队列
Specific Schedulers（特定调度器） 针对不同任务类型的具体实现：
- DL scheduler（Deadline调度器）
- RT scheduler（实时调度器）
- CFS scheduler（普通进程调度器）
- Idle scheduler（空闲进程调度器）
- Custom scheduler（可自定义扩展）

== Linux/FreeBSD BFS 调度

=== BFS调度器

*BFS 的思路*
- BFS全称：Brain Fuck Scheduler，脑残调度器
- BFS 调度算法是一种*时间片轮转算法的变种*
- 在多处理机时使用单就绪队列（双向链表）
  - 增加了队列互斥访问的开销
  - 减少了负载均衡算法开销

  #three-line-table[
    | 特性     | BFS             | CFS        |
    | ------ | --------------- | ---------- |
    | 设计目标   | 低延迟、简单、可预测      | 公平性、高吞吐    |
    | 核心数据结构 | 单一就绪队列（双向链表）    | 每CPU一棵红黑树  |
    | 多核策略   | 所有CPU共享就绪队列     | 每核独立运行队列   |
    | 复杂度    | 简单易实现           | 较复杂        |
    | 调度开销   | 可能高于CFS（队列锁争用）  | 较低         |
    | 调度粒度   | 时间片固定（1~1000ms） | 动态时间片（按权重） |
  ]

*BFS 的进程优先级*
- 进程有 103 个优先级
  - 100 个静态的实时优先级；
  - 3 个普通优先级
    - `SCHED_ISO` (isochronous) : 交互式任务
    - `SCHED_NORMAL` : 普通任务
    - `SCHED_IDLEPRIO` ：低优先级任务

*BFS 的就绪队列*
- 就绪队列
  - 所有 CPU 共享一个*双向链表结构的单就绪队列*
  - 所有进程按优先级排队
  - 相同优先级的进程根据虚拟截止时间 (Virtual Deadline) 排序
  - 相同优先级的每个进程有一个时间片长度和虚拟截止时间
*BFS 的时间片*
- 时间片大小
  - 由算法参数指定，可在 1ms 到 1000ms 间选择，缺省设置为 6ms
- *虚拟截止时间*（Virtual Deadline）：关于就绪队列中进程等待 CPU 最长时间的排序，并不是真实的截止时间
  - 进程*时间片用完*时，重新计算虚拟截止时间
  - *事件等待结束*时，虚拟截止时间保持不变，以抢先相同优先级的就绪进程
  - 为了让进程在*上次运行的 CPU* 上运行（亲和性），不同 CPU 对进程的虚拟截止时间加一个权重
- BFS 的虚拟截止时间计算
  - 依据当前时间、进程优先级和时间片设置计算
    ```
    offset = niffies + (prio_ratio ∗ rr_interval)
    prioratio increases by 10% for every nice level
    ```
    - `niffies`：当前时间
    - `prio_ratio`：进程优先级对应的权重
    - `rr_interval`：时间片长度
    - nice值每增加1，prio_ratio增加约10%
*BFS 的调度策略*
- BFS 维护 103 个优先级队列
- 使用类似 O(1) 调度器的 位图 (bitmap) 查找
- 选出最高优先级的非空队列
- 在该队列中以 O(n) 查找最小虚拟截止时间的任务
- 选出该任务运行
  #figure(
    image("pic/2025-10-29-20-16-57.png", width: 80%),
    numbering: none,
  )

*BFS 的就绪队列插入*
- 时间片用完：重新设置虚拟截止时间后，插入就绪队列
- 等待事件出现：虚拟截止时间保持不变，抢先低优先级进程或插入就绪队列

*BFS 的性能取舍*
- #three-line-table[
    | 目标    | BFS 设计选择      | 结果         |
    | ----- | ------------- | ---------- |
    | 响应延迟  | 单队列 + 唤醒抢占    | 极低延迟（适合桌面） |
    | 吞吐性能  | 没有负载均衡机制      | 多核扩展性差     |
    | 公平性   | 基于优先级与VD      | 简单但非精确公平   |
    | 实现复杂度 | 极低            | 易维护、易理解    |
    | 场景适用性 | 桌面 / 嵌入式 / 单机 | 不适合高负载服务器  |
  ]

*与 CFS 调度器对比*

- #three-line-table[
    | 对比项   | CFS             | BFS           |
    | ----- | --------------- | ------------- |
    | 数据结构  | 红黑树 per-CPU     | 单链表全局队列       |
    | 调度复杂度 | O(log N)        | O(N)          |
    | 公平性   | 按 vruntime 精确公平 | 按优先级+VD 近似公平  |
    | 响应延迟  | 中等              | 极低            |
    | 多核扩展  | 强               | 弱             |
    | 实时性能  | 一般              | 强（有SCHED_ISO） |
    | 适用场景  | 服务器、大规模并发       | 桌面、交互式系统      |
  ]

=== BFS 与 CFS 的性能对比 (2012)

*BFS 与 CFS 的性能对比实验*
- 操作系统内核版本：Linux kernel 3.6.2（CFS）
和 Linux kernel 3.6.2.2（BFS）
- 测试平台：多核桌面系统
- 调度器：CFS vs BFS
- 测试类型（Benchmark suite）：
  - GCC 编译测试 → CPU 密集型
  - 内核源代码压缩（lrzip） → 混合 I/O 与 CPU 负载
  - 视频编码测试（ffmpeg 720p→360p） → 多媒体交互型任务，测试响应延迟与吞吐量
- BFS 与 CFS 的性能差异分析
  #three-line-table[
    | 测试类型                   | CFS 表现 | BFS 表现        | 对比说明                         |
    | ---------------------- | ------ | ------------- | ---------------------------- |
    | *GCC 编译 (CPU-bound)* | 吞吐量略高  | 吞吐量略低 (~2–5%) | CFS 在长时批处理任务中更有优势            |
    | *内核压缩 (lrzip)*       | 平均性能   | 相近或略优         | BFS 的队列竞争影响较小                |
    | *视频编码 (ffmpeg)*      | 响应延迟偏高 | 延迟显著降低，响应更流畅  | BFS 优化交互任务调度，延迟下降 20–30%     |
    | *交互体验（桌面操作）*         | 普通响应   | 明显更平滑，无“卡顿”   | BFS 的“单队列+唤醒优先”机制使得GUI操作极快响应 |
  ]

*最早可执行虚拟截止时间优先调度算法EEVDF*
- EEVDF（Earliest Eligible Virtual Deadline First）调度算法
  - EEVDF = CFS + Deadline 思想的融合
  - 在保持 CFS 的红黑树 + per-CPU 框架的同时，引入 BFS 的“虚拟截止时间 (Virtual Deadline)” 概念，让调度器既公平又快速响应
